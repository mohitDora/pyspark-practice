{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a2d6bf4-706f-4bca-ade7-8d02734fcb22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Turning off AQE (Adaptive query execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50fe3c2c-0255-469c-aed5-c726ce13e90e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39ed1577-7ebc-495f-8802-74f05a70dab7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.adaptive.enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6a504f1-c910-40c1-857c-280d57aea8f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39c8df97-4467-488f-bcc5-24947a2977c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"/FileStore/tables/BigMart_Sales.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d6c30f-23cf-449b-a097-a896cf68a53d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22639d56-aa81-40ad-87a0-1bd3037bac19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Spark divides the file into partitons. By default partition size is 128 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "148fc11d-2800-441e-b7c6-808688dcfd57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()\n",
    "# in our case file size is just ~800 KB, thats why number of partition is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e5446e6-1a2e-46fc-a3c1-2ec8b9818dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\",131072)\n",
    "#setting partition size to 128kb explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d566adef-2b79-40fd-ba0a-0af742418094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\",134217728)\n",
    "#changing to default 128 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce166205-f3b8-4bc9-ba8b-c41d8c3bed94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Repartitioning - Partitioning w/o changing the default partition size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d5bd0c3-78b1-476b-922e-918bc07a1adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5cd184-ca36-44c0-a809-c5fcbf35003b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a73b558-d91b-46ae-9c5f-ff94c662f1ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Getting partition info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a58edf-852b-4771-8071-2b86ecafe90a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn(\"partition_id\",spark_partition_id()).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a18c710e-45b6-4f57-9a78-22425341ad03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Saving into parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7e57fd9-5113-43b4-80ca-7a577492c9d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").mode(\"append\").save(\"/FileStore/parquet/\")\n",
    "#Write 10 parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09b28cc0-e7e6-4ac4-b240-055339acf2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new=spark.read.parquet(\"/FileStore/parquet/\")\n",
    "#Reading 10 files\n",
    "df_new.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8082ef3-e7b5-4662-b7ac-649fa750a185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new.filter(col(\"Outlet_Location_Type\")==\"Tier 2\").display()\n",
    "#here we are getting 1/3 of data, but for that we have to scan all 10 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f2c46dc-f2e2-4eed-8182-bcf9855ac255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Scanning Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08bdcb80-9c4b-483e-972b-1618e5b05387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").mode(\"append\").partitionBy(\"Outlet_Location_Type\").save(\"/FileStore/parquet/opt/\")\n",
    "#store parquet files partitioned by Outlet_Location_Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe177b1-7268-43eb-9a8d-67c04b384096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new_2=spark.read.parquet(\"/FileStore/parquet/opt\")\n",
    "#Reading 10 files\n",
    "df_new_2.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3fb58f2-37ab-4526-a01b-040dce940bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new_2.filter(col(\"Outlet_Location_Type\")==\"Tier 2\").display()\n",
    "#Now the executor have power to choose which partition to take and which to not take\n",
    "#Its always better to partition files on date, i.e year, month "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "429d71c1-a6c3-4b98-980f-4443212973d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Partitioning refers to the division of your data into smaller, manageable chunks that can be processed in parallel across your cluster.\n",
    "\n",
    "Number of Partitions: Directly impacts parallelism. Too few, and you underutilize your cluster. Too many, and overhead from task scheduling and managing small files outweighs benefits.\n",
    "\n",
    "repartition() vs. coalesce():\n",
    "\n",
    "repartition() - performs a full shuffle (Data is redistributed across the cluster). Can increase/ decrease partitions\n",
    "\n",
    "coalesce() - It tries to merge existing partitions. only decrease the partitions.\n",
    "\n",
    "Use when:\n",
    "You have too many partitions (e.g., after filtering) and want to reduce the number of output files (the \"small file problem\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe8a4c23-dcb5-4cf7-9f83-d7ee9dd010c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Shuffle - It is an expensive operation in spark where data is redistributed across partitions. It occurs when a transformation requires data from different partitions to be grouped or aggregated together.\n",
    "\n",
    "ex - groupBy, join, orderBy, repartition\n",
    "\n",
    "Why are Shuffles Expensive?\n",
    "\n",
    "Disk I/O: Data is written to disk by mappers and read from disk by reducers.\n",
    "Network I/O: Data is transferred across the network between worker nodes.\n",
    "Serialization/Deserialization: Data needs to be serialized before sending and deserialized upon receipt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba0a1540-2344-4973-8189-9f411d833cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Joining Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26118283-4546-448d-8205-15be33c58a3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "While joining everytime the records shuffles in some ~200 partition and then executor executes the transformation. Very expansive opeartion.\n",
    "\n",
    "SOL: Broadcast Join\n",
    "\n",
    "only works when one df is small around ~5-10 MB, such that it can easily broadcast to executors by driver program.\n",
    "\n",
    "So, in this, distribute the partition of large df among all executors.\n",
    "\n",
    "Then the driver broadcast the small df to all executors.\n",
    "\n",
    "In this way we prevent from shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e609042-05ef-49f8-9605-698992b94098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#small df\n",
    "data_customers = [\n",
    "    (1, \"Alice\", \"USA\"),\n",
    "    (2, \"Bob\", \"UK\"),\n",
    "    (3, \"Charlie\", \"Canada\")\n",
    "]\n",
    "columns_customers = [\"customer_id\", \"name\", \"country\"]\n",
    "\n",
    "df_customers = spark.createDataFrame(data_customers, columns_customers)\n",
    "\n",
    "#Big df\n",
    "data_orders = [\n",
    "    (101, 1, 250),\n",
    "    (102, 2, 450),\n",
    "    (103, 1, 300)\n",
    "]\n",
    "columns_orders = [\"order_id\", \"customer_id\", \"amount\"]\n",
    "\n",
    "df_orders = spark.createDataFrame(data_orders, columns_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43639fc8-5422-46ff-80e5-cae7497744de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers.display()\n",
    "df_orders.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54289d2f-8e4e-4edc-a0e1-f9e605dfebc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#normal join\n",
    "df_orders.join(df_customers,df_customers[\"customer_id\"]==df_orders[\"customer_id\"],\"left\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "628aaa60-9707-4eec-a268-f029d8103ba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#optimised join\n",
    "df_orders.join(broadcast(df_customers),df_customers[\"customer_id\"]==df_orders[\"customer_id\"],\"left\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323ba600-c718-46af-aa01-713675e1e3fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "df_orders.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "747f9034-279a-4440-8801-8f2e7929a4fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#SQL hints - provides hints to sql to perform such thing\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT /*+ BROADCAST(c) */ \n",
    "    * FROM\n",
    "    customers c\n",
    "    JOIN orders o\n",
    "    ON c.customer_id = o.customer_id\n",
    "    \"\"\"\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d885524-fb33-43e2-9cd1-46c5c96eced0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Caching and persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b28f2d24-84ab-4972-bd6a-f5486ccd4a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "cache(): A shorthand for persist(StorageLevel.MEMORY_AND_DISK). It tries to store the DataFrame/RDD in memory, and if memory is insufficient, it spills to disk.\n",
    "\n",
    "StorageLevel : MEMORY_AND_DISK, MEMORY_ONLY, DISK_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3699b89-4857-4ded-b7f7-79e4f4465469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2=spark.read.csv(\"/FileStore/tables/BigMart_Sales.csv\",header=True,inferSchema=True).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e343c10d-fcf4-4306-bdf3-41a5af973e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dynamic resource allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec50a345-82f8-4b16-ba2f-4aa1582d8f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Instead of capturing all resource, we can dynamically scale down or up according to need\n",
    "\n",
    "spark.dynamicAllocation.enabled = true\n",
    "\n",
    "spark.dynamicAllocation.minExecutors = 1\n",
    "\n",
    "spark.dynamicAllocation.maxExecutors = 10\n",
    "\n",
    "spark.dynamicAllocation.initialExecutors = 2\n",
    "\n",
    "spark.shuffle.service.enabled = true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1d8de89-d0c6-4b7a-b5fd-48d00bb5f246",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adaptive Query Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1cf4c38-b805-4136-bdde-a928b83b1b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It is a performance optimization feature that dynamically optimizes query plans at runtime based on actual data \n",
    "\n",
    "Characteristics: \n",
    "\n",
    "1. Dynamic Join Strategy Selection :  Automatically switches join strategies (e.g., broadcast vs. shuffle join) based on actual data sizes.\n",
    "\n",
    "2. Skew Join Optimization : Detects skewed partitions and breaks them into smaller ones to balance work across executors.\n",
    "\n",
    "3. Coalescing Shuffle Partitions : Dynamically reduces the number of shuffle partitions at runtime to avoid small or empty partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93f8effb-2b3e-4ed9-aa27-1a36b2ce42bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dynamic Partition Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3423ea2-d47b-44e0-b62f-f351d698f801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dynamic Partition Pruning (DPP) is a powerful optimization feature in Apache Spark that automatically prunes unnecessary partitions at runtime, reducing I/O and improving query performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "505c3004-ab56-4534-9bc4-8c04360ac954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When you join a fact table (large, partitioned) with a dimension table (filtering values), Spark dynamically determines which partitions to read based on the actual join keys during execution, instead of reading all partitions.\n",
    "\n",
    "SELECT *\n",
    "FROM sales s\n",
    "JOIN regions r\n",
    "  ON s.region = r.region\n",
    "WHERE r.country = 'USA'\n",
    "\n",
    "**IMPORTANT**\n",
    "The data should be partitioned on the same column, the joining condition is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa03c61a-93d7-4ac7-b972-a21607a1a92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.optimizer.dynamicPartitionPruning.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18219b17-772b-4ddf-a2a9-7776653e674a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_not_partitioned=spark.read.csv(\"/FileStore/tables/BigMart_Sales.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d86ab742-55b7-4823-a96b-e42d040af91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_new_2.join(df_not_partitioned.filter(col(\"Outlet_Location_Type\")==\"Tier 3\"),df_not_partitioned[\"Outlet_Location_Type\"]==df_new_2[\"Outlet_Location_Type\"],\"inner\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c66f626-db36-484c-8a3a-9aa512eb5d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"Outlet_Location_Type\").count().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6be872e5-35cb-4dc3-b897-dd56b33f1ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Broadcast Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57a83d48-984f-4f72-a05a-bbcd21e2f64b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Broadcast variables allow you to efficiently distribute read-only variable (like a lookup table or a small DataFrame) to all worker nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7695403-d817-422b-85da-02ce1a7a5b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# look-up dictionary\n",
    "product_lookup_data = {\n",
    "\"prod_0\":\"Laptop\",\n",
    "\"prod_1\":\"Mouse\",\n",
    "\"prod_2\":\"Shirt\"\n",
    "}\n",
    "\n",
    "df_products=spark.createDataFrame([\n",
    "    (\"prod_0\",),\n",
    "    (\"prod_1\",),\n",
    "    (\"prod_2\",),\n",
    "],[\"product_id\"])\n",
    "\n",
    "df_products.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fcf5b86-e193-419d-9bcc-bf2757286e2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "broad_products=spark.sparkContext.broadcast(product_lookup_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d7bd6a-2cc5-47fc-95cb-8e066dc903f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "broad_products.value.get(\"prod_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c6528ee-58b2-4e41-937d-cd9b7f73d06d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def my_func(x):\n",
    "    return broad_products.value.get(x)\n",
    "\n",
    "my_udf=udf(my_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2947391d-3e89-4742-b5be-bcae6f6b229e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_products.withColumn(\"product_name\",my_udf(col(\"product_id\"))).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e1d8225-80f3-4a1f-a4c0-afffaa714db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Salting OOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7908d6d3-1ee4-4105-821b-e80577fe73e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Salting is a data engineering technique used to prevent data skew—where a small number of keys dominate the dataset and cause certain tasks to consume too much memory (OOM = Out Of Memory).\n",
    "\n",
    "Appling a random or deterministic \"salt\" value to the skewed key so that the load is spread more evenly across partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a99e18-c2cd-44fa-b399-56445353f682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_skew=spark.createDataFrame([\n",
    "  (1,700),\n",
    "  (2,200),\n",
    "  (3,300),\n",
    "  (2,600),\n",
    "  (1,150),\n",
    "  (1,250),\n",
    "  (1,300),\n",
    "],[\"cust_id\",\"amount\"])\n",
    "\n",
    "df_skew.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ca6a35-cb53-4d33-9fce-13e0c6e50cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_skew=df_skew.withColumn(\"salt\",floor(rand()*3))\n",
    "df_skew=df_skew.withColumn(\"cust_id_salt\",concat(\"cust_id\",lit(\"-\"),\"salt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d4fdf47-43f6-478a-b6d9-6368487c2b96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_skew.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6d0217e-2562-42c7-8157-11cd61f3e6f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_skew.groupBy(\"cust_id_salt\").sum(\"amount\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark-optimization",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}