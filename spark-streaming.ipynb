{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46bd4bde-34e3-474a-ab6e-87c9ce6b8bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91f83954-ecbb-4c26-b783-67bd32228403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Spark Streaming (DStreams) :** The older, original streaming API based on DStreams (Discretized Streams). It processes data in micro-batches, but its API is somewhat separate from the DataFrame API.\n",
    "\n",
    "\n",
    "**Spark Structured Streaming (Modern) :** Built on top of the Spark SQL engine and DataFrames. It treats a live data stream as a continuously appending table.\n",
    "\n",
    "**Key concepts :**\n",
    "\n",
    "**Continuous Table Analogy :** Structured Streaming treats incoming data streams as a table that is being continuously appended. Each record that arrives is like a new row being added to this \"unbounded\" table.\n",
    "\n",
    "**Micro-batch processing :** Structured Streaming processes data in small, continuous batches.  Data is collected for a short duration (e.g., 1 second) and then processed as a mini-batch Spark job\n",
    "\n",
    "**Fault tolerant**\n",
    "\n",
    "**Stateless vs stateful transformations**\n",
    "\n",
    "Stateless : These transformations operate on each micro-batch independently, without needing information from previous batches. e.g select, withColumn, filters, union\n",
    "\n",
    "stateful : These operations require maintaining state across multiple micro-batches. e.g group by\n",
    "\n",
    "output modes-append, update, complete\n",
    "\n",
    "append mode only work with stateless mode, use case : logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f269e67-721e-4770-a823-590bc7dd8b83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Checkpointing\n",
    "\n",
    "Checkpointing ensures fault tolerance and Idempotency by maintaining state in a persistent location called checkpoint directory\n",
    "\n",
    "Offset tracking : for kafka stores the offset that it has successfully processed and for file, it stores which files have been processed. So on resume it start from that offest.\n",
    "\n",
    "Schema and Query plan : stores the logical/physical plan and schema of the dataframe.\n",
    "\n",
    "Folder Structure :\n",
    "commits : tracks if the file has been successfully processed or not\n",
    "offsets : tracks the offests\n",
    "sources : have files that are successfullt processed\n",
    "metadata : about the query\n",
    "state (if stateful opreations are used) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cae441f-341e-4ead-967e-9204137cf400",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### How Checkpointing Works\n",
    "\n",
    "Define Query: You define your streaming query (source, transformations, sink) and specify a checkpointLocation.\n",
    "\n",
    "Start Query: When you start the query, Spark checks the checkpointLocation.\n",
    "If it's empty: Spark starts the query from scratch, records initial offsets, and builds initial state (if any).\n",
    "If it contains previous checkpoint data: Spark reads the latest checkpoint information, restores the query's state, and picks up processing from the recorded offsets.\n",
    "\n",
    "Process Micro-batches:\n",
    "For each micro-batch, Spark reads new data from the source.\n",
    "It processes the data through transformations.\n",
    "It updates the internal state (for stateful operations).\n",
    "Before writing results to the sink, Spark writes the updated offsets and state to the checkpoint directory.\n",
    "Only after the checkpoint data is successfully written, Spark commits the output to the sink.\n",
    "\n",
    "Fault/Restart: If the application crashes at any point, when you restart the query with the same checkpointLocation:\n",
    "Spark reads the latest complete checkpoint.\n",
    "It knows precisely which data has been processed (by source offsets) and what the state was.\n",
    "It resumes processing from the point of the last successful checkpoint, potentially replaying some data from the source if it was processed but not yet committed to the sink before the crash. This \"replay\" combined with idempotent sinks ensures exactly-once delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a6027d-dac8-46ed-ba44-8429553753aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading JSON file (Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61bb615d-050f-46ec-a2e1-0d5a0238aa04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9539679-5369-4c38-83e5-6819f6c61aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"json\")\\\n",
    "        .option(\"inferSchema\",True)\\\n",
    "        .option(\"multiline\",\"true\")\\\n",
    "        .load(\"/Volumes/workspace/my-schema/my-volume/json/day1.json\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74f9e512-0145-40f5-a666-d8a37171e57a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.select(\"order_id\",\"customer.customer_id\",\"customer.email\",\"customer.name\",\"customer.address.city\",\"customer.address.country\",\"customer.address.postal_code\",\"payment.method\",\"payment.transaction_id\",\"timestamp\",\"metadata\",\"items\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5466ff8-2257-4a7e-bb68-0047de046dac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.withColumn(\"metadata\",explode_outer(\"metadata\")).withColumn(\"items\",explode_outer(\"items\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095bbcac-e274-4651-85e1-ff44625dee52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.select(\"*\",\"items.item_id\",\"items.price\",\"items.product_name\",\"items.quantity\",\"metadata.key\",\"metadata.value\").drop(\"items\",\"metadata\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "326835db-838a-4f71-ae6e-2937fe962997",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Streaming json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc9deb4b-c304-4819-968a-c8eeb14735b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"customer\", StructType([\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"email\", StringType(), True),\n",
    "        StructField(\"address\", StructType([\n",
    "            StructField(\"city\", StringType(), True),\n",
    "            StructField(\"postal_code\", StringType(), True),\n",
    "            StructField(\"country\", StringType(), True)\n",
    "        ]))\n",
    "    ])),\n",
    "    StructField(\"items\", ArrayType(StructType([\n",
    "        StructField(\"item_id\", StringType(), True),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"price\", DoubleType(), True)\n",
    "    ]))),\n",
    "    StructField(\"payment\", StructType([\n",
    "        StructField(\"method\", StringType(), True),\n",
    "        StructField(\"transaction_id\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"metadata\", ArrayType(StructType([\n",
    "        StructField(\"key\", StringType(), True),\n",
    "        StructField(\"value\", StringType(), True)\n",
    "    ])))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b9ac720-7a08-47d9-953e-96feaeaf913c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_df=spark.readStream.format(\"json\")\\\n",
    "        .option(\"multiline\",\"true\")\\\n",
    "        .schema(schema)\\\n",
    "        .load(\"/Volumes/workspace/my-schema/my-volume/json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73146c08-49f2-4bbd-b9c8-8506a63965cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_df=_df.select(\"order_id\",\"customer.customer_id\",\"customer.email\",\"customer.name\",\"customer.address.city\",\"customer.address.country\",\"customer.address.postal_code\",\"payment.method\",\"payment.transaction_id\",\"timestamp\",\"metadata\",\"items\")\n",
    "\n",
    "_df=_df.withColumn(\"metadata\",explode_outer(\"metadata\")).withColumn(\"items\",explode_outer(\"items\"))\n",
    "\n",
    "_df=_df.select(\"*\",\"items.item_id\",\"items.price\",\"items.product_name\",\"items.quantity\",\"metadata.key\",\"metadata.value\").drop(\"items\",\"metadata\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff225c1c-3ab7-425a-828f-d160b65e85f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When we write, only the new data get transformed and stored -> Idempotency (Process data not process again). This only possible because of saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8de2027a-2094-4598-bdf1-4271600b7fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_df.writeStream.format(\"delta\")\\\n",
    "    .trigger(once=True)\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"path\",\"/Volumes/workspace/my-schema/my-volume/sink/data\")\\\n",
    "    .option(\"checkpointLocation\",\"/Volumes/workspace/my-schema/my-volume/sink/checkpoint\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f767c21e-af69-4e00-ac79-491ac181ae12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reading data from sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c9bc139-4746-445f-8a6b-b52e1a4b9e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * from delta.`/Volumes/workspace/my-schema/my-volume/sink/data/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13f1efbd-ad6b-42b3-8b18-c1b337b38126",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Triggers\n",
    "\n",
    "It defines when the next micro batch should be executed\n",
    "\n",
    "**Types :**\n",
    "\n",
    "**default** - When not mention any trigger, it uses default one. In this next batch executed just after the current batch processed.\n",
    "\n",
    "**processingTime** - It instructs spark to execute the batch after a regular user-defined interval.\n",
    "\n",
    ".trigger(processingTime=\"interval string\")\n",
    "interval string examples: \"5 seconds\", \"1 minute\", \"10 minutes\", \"1 hour\".\n",
    "\n",
    "**once** - instructs spark to process all currently available data from the source and then shut down.\n",
    "\n",
    ".trigger(once=True)\n",
    "\n",
    "**available now** - It also process all data present in the source but divide that data in micro batch. Useful when source data is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "667b7de0-8545-4bd0-8fc9-f2dbbb3c14e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Archiving source files\n",
    "\n",
    "When the file present in the source get processed, we generally archive that file only when a new file come and the new file should not be already processed one.\n",
    "\n",
    "If the file is processed and file reuploaded that is processed already. Then the new file is not processed and hence the processed file remain in the source until a new file come. And the already processed file will remail there in the source like garbage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fe3599a-3863-4298-9ba7-5146f18ac315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " __df=spark.readStream.format(\"json\")\\\n",
    "        .option(\"multiline\",\"true\")\\\n",
    "        .option(\"cleanSource\",\"archive\")\\\n",
    "        .option(\"SourceArchiveDir\",\"/Volumes/workspace/my-schema/my-volume/archive\")\\\n",
    "        .schema(schema)\\\n",
    "        .load(\"/Volumes/workspace/my-schema/my-volume/json_new/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1bc7c6-8c86-4e85-8298-c0cb5ed150ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "__df=__df.select(\"order_id\",\"customer.customer_id\",\"customer.email\",\"customer.name\",\"customer.address.city\",\"customer.address.country\",\"customer.address.postal_code\",\"payment.method\",\"payment.transaction_id\",\"timestamp\",\"metadata\",\"items\")\n",
    "\n",
    "__df=__df.withColumn(\"metadata\",explode_outer(\"metadata\")).withColumn(\"items\",explode_outer(\"items\"))\n",
    "\n",
    "__df=__df.select(\"*\",\"items.item_id\",\"items.price\",\"items.product_name\",\"items.quantity\",\"metadata.key\",\"metadata.value\").drop(\"items\",\"metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be6680b1-d178-47d6-992b-333fc5c57fca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "__df.writeStream.format(\"delta\")\\\n",
    "    .trigger(once=True)\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"path\",\"/Volumes/workspace/my-schema/my-volume/sink_new/data\")\\\n",
    "    .option(\"checkpointLocation\",\"/Volumes/workspace/my-schema/my-volume/sink_new/checkpoint\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57f5b336-c430-429a-814b-359f93ee4699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * from delta.`/Volumes/workspace/my-schema/my-volume/sink_new/data/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9827304-a291-4599-9e11-b3a8f880b7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Output modes\n",
    "\n",
    "**append** (Default for stateless): Only new rows appended to the results table since the last trigger are written to the sink. This is the default and most efficient for stateless queries.\n",
    "Requirement for stateful queries: If your query is stateful (e.g., aggregation), it needs a watermark defined to use append mode efficiently.\n",
    "\n",
    "**complete**: The entire updated result table is written to the sink after each trigger. All aggregated results (e.g., all word counts) are re-written. This is often used for aggregations where you want to see the full, current state.\n",
    "Caveat: Can be inefficient for very large state, as the entire state is rewritten.\n",
    "\n",
    "**update** (Spark 2.1+): Only rows that have been updated in the results table since the last trigger are written to the sink. This is more efficient than complete for stateful queries where only a few rows change.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a55000-1996-426d-ad7d-03df86bf9c89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ForEachBatch\n",
    "\n",
    "def my_func(df, bach_id):\n",
    "\n",
    "  df.write.format()\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"path\",\"...\")\\\n",
    "    .save()\n",
    "\n",
    "  df.write.format()\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"path\",\"...\")\\\n",
    "    .save()\n",
    "\n",
    "df.writeStream.foreachBatch(my_func)\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .trigger(once=True)\\\n",
    "    .option(\"checkPointLocation\",\"...\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0d84583-cf7e-4133-9010-f8f3fa8ef8e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Event vs processing time\n",
    "\n",
    "**processing time**\n",
    "\n",
    "It refers to the time on the system that is processing the data.\n",
    "\n",
    "**problems with this :**\n",
    "\n",
    "**Out-of-Order Data:** Events might arrive in a different order than they occurred.\n",
    "Example: Sensor A sends a reading at 9:00 AM. Sensor B sends a reading at 9:01 AM. Due to network congestion, Sensor A's reading arrives at 9:03 AM, and Sensor B's reading arrives at 9:02 AM. If you're doing a 1-minute windowed count based on processing time, Sensor B's 9:01 AM event might get counted in the 9:02-9:03 window, while Sensor A's 9:00 AM event (arriving late) might fall into the 9:03-9:04 window, leading to incorrect counts for those windows.\n",
    "\n",
    "use only in stateless transformations and when needed low latency\n",
    "\n",
    "**Event time**\n",
    "\n",
    "It refers to the time the event actually occured.\n",
    "\n",
    "Benefits :\n",
    "\n",
    "Correct windowing, Accurate aggregations\n",
    "\n",
    "use when needed time-based aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ee80c94-5479-45ea-a998-75948080cd42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Window Opearions\n",
    "\n",
    "It allows us to aggragate data into window of time.\n",
    "\n",
    "Types : \n",
    "\n",
    "Tumbling window : Fixed size non overlapping window\n",
    "Sliding window : fixed size overlapping window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089c93d7-ce5a-40bd-9c52-2ecc9ffd1765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE workspace.`my-schema`.myTable(\n",
    "  color STRING,\n",
    "  event_date TIMESTAMP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bedc73d-a69e-42e1-9b5e-387ef3dd24ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "INSERT INTO workspace.`my-schema`.myTable VALUES\n",
    "('red',   TIMESTAMP('2025-06-22 10:04:45'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c25cc70-4705-4cfc-84a5-47b7577193c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_table=spark.readStream.table(\"workspace.`my-schema`.myTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d73c3381-064c-4c4a-b718-e97bd609b6a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_table=df_table.withWatermark(\"event_date\", \"10 seconds\")\n",
    "df_table=df_table.groupBy(\"color\",window(\"event_date\", \"5 minutes\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d098193-39bd-4434-8a4c-f5e0768e8437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_table.writeStream.format(\"delta\")\\\n",
    "    .trigger(once=True)\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .option(\"checkpointLocation\",\"/Volumes/workspace/my-schema/my-volume/table/checkpoint\")\\\n",
    "    .option(\"path\",\"/Volumes/workspace/my-schema/my-volume/table/sink\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74c42fb-9b45-4557-b79b-241b8d37d28f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM DELTA.`/Volumes/workspace/my-schema/my-volume/table/sink/`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8986100872200520,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}